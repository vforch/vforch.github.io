---
layout: post
title: "The Complexity Limit of LLMs as Innovation Engines for Machine Learning - Can we find Move 37 for Neural Architecture Search?"
date: 2025-10-01
tags: [complexity, evolution, LLMs]
---

The use of LLMs for innovation may boost productivity in some domains, but can they generate the breakthrough insights needed to revolutionize machine learning itself? Or is there a limit on the complexity of the ideas LLMs can work with due to being constrained by the human knowledge they were trained on?

---

Since 2022, LLMs have shown some impressive or at least unexpected capabilities. Their "thinking" or "reasoning" modes, which can be understood as a self-guided shallow search over possible responses, further boosted their capabilities. Recently, LLMs have been used as heuristics for generating and evaluating candidate solutions in large search spaces embedded in the language domain. This approach has already produced some significant feats with potential real-world impact:

- Using a mix of proprietary LLMs for literature search and information extraction, researchers created an [automated workflow for systematic medical reviews](https://www.medrxiv.org/content/10.1101/2025.06.13.25329541v2). These reviews are very labour-intensive, taking months to complete. The automated workflow, called otto-SR, reduces the time it takes to write such a review to hours (given some detailed, pre-specified inclusion criteria for papers), cutting delays in knowledge accumulation and allowing for frequent updates. Further, across the twelve reviews the authors reproduced, their automatic approach identified more relevant studies and was more reliable at data extraction than human reviewers.
- In an apparent world first, [Sakana's](https://sakana.ai/) AI scientist "published" its [first peer-reviewed paper](https://sakana.ai/ai-scientist-first-publication/).[1](#footnote-1) The algorithm employs multiple automatic evaluation functions—including LLM judges—to iteratively improve its LLM-generated research ideas as well as their realization in code, graphs, and text. While far from perfect, the paper was deemed "above acceptance threshold" for a workshop paper at a leading machine learning conference. This signifies a serious shift for the scientific community where prestige (and therefore tenure and funding) is largely traded in publication scores. After all, GPU-powered LLMs will likely scale faster than LLM-assisted researchers.

Many of these mini-breakthroughs are occuring[2](#footnote-2) in domains that interface with language. Since machine learning is mostly underpinned by writing high-level algorithmic descriptions of neural network inference and training, this raises the prospect of letting LLMs re-write their own code, automating most of machine learning research and thus leading to self-improving models. In theory,  this could culminate in an AI "singularity"—a point where machine intelligence *suddenly* outclasses human intelligence. If AI were then to spill over into the real world, this would have unpredictable consequences for humanity and life on earth.

The idea of self-replicating and self-improving machines traces back to the [19th century](https://arxiv.org/pdf/1806.01322), when Charles Darwin introduced the concept of Evolution through natural selection. His revolutionary theory of black box optimization merged naturally with fears raised by the Industrial Revolution's replacement of human labour with untiring machines. This line of thinking resurfaced in early science fiction and was later popularised by Ray Kurzweil's 2005 book "The Singularity Is Near", which argued that exponential improvements in available compute (Moores law etc.) would directly accelerate the algorithmic search for better AI. AI superintelligence also found its way into academic thinking - first as [speculation](https://doi.org/10.1016/S0065-2458(08)60418-0) and later [formalized](https://people.idsia.ch/~juergen/goedelmachine.html) by Jürgen Schmidhuber as the Gödel machine, which must first prove the optimality of every potential self-improvement. 

However, not all experts share this vision. [Francois Chollet](https://youtu.be/Bo8MY4JpiXE?t=115), for example, disputes the narrative of exponential self-improvement, arguing that there are diminishing returns and fundamental forces of friction or bottlenecks even super smart machines cannot circumvent. Others, like [Yann Lecun](https://www.youtube.com/watch?v=4__gg83s_Do) and [Richard Sutton](https://www.youtube.com/watch?v=21EYKqUsPfg), deny that current transformer-based LLMs could ever scale to anything approaching human intelligence. Matters are further complicated by an unclear definition of the goal. How do we define super intelligence - or human intelligence for that matter - and do we even want something that resembles our own intelligence? 

But let's go back to what is on the horizon in the automatic search department. Sakana introduced a self-rewriting LLM agent called the [Darwin Gödel Machine](https://sakana.ai/dgm/), which blends Schmidhuber's theoretically optimal but impractical Gödel machine with the fuzziness of evolutionary search. Rather than proving that a change to its codebase is an improvement, the DGM uses empricial performance estimates to decide which change to make. To avoid local optima, it maintains a repertoire of different "sub-optimal" solutions. This deliberate diversity allows it to make some "detours" in the optimization landscape that eventually lead to better outcomes. While the DGM has been tested only on two coding benchmarks, the results lend some credence to the idea of self-improvement—the DGM outperformed a control setup where a static meta-agent edited a coding agent but couldn't change its own code. However, even in the DGM, the LLM used for code editing and making test-time inferences was frozen—it could not be changed by the agent. Retraining an LLM every time you change its architecture is currently far too expensive to be done at scale.

At a smaller scale, however, experimenting with neural network architectures is possible. This field, called [neural architecture search](https://www.jmlr.org/papers/v20/18-598.html) (NAS), is a large part of what researchers in the machine learning community are doing—often manually by trial and error—just like in [Evolution](https://arxiv.org/abs/2205.10320). Of course, NAS is increasingly being automated using LLMs. A research group based in Shanghai even claims to have created a process capable of finding novel neural architectures humans could not have come up with—analogous to [AlphaGo's Move 37](https://youtu.be/WXuK6gekU1Y?t=2969) which first appeared to be a mistake to the human observer but proved to be a new, ingenious variant humans had never considered during centuries of playing Go. Embracing the singularity vibes, they named their LLM-powered NAS [Artificial Superintelligence for AI research](https://arxiv.org/abs/2507.18074) (ASI-Arch). 

This approach leverages LLMs to propose, implement, and test incremental modifications to explore a search tree over model architectures—similar to what the AI scientist does to produce research papers. Investing merely 20.000 GPU hours—roughly equivalent to the monthly salary of a research scientist in electricity cost—they discovered over 100 new, functioning model architectures with linear attention mechanisms[3](#footnote-3). Interestingly, the scoring metric included not just raw benchamrk performance but also an LLM-based rating of the architectural "merit": its novelty, complexity, and speed of convergence during training.

To make the search computationally feasible, the tested models were constraint to 340 million parameters—less than 1/1000th the size of proprietary LLMs. However, the [breakthrough transformer paper](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) that launched the LLM revolution, also used models roughly this size. So it is conceivable that the automatic search for new architectures may lead to a new generation of LLMs, right? 

This brings us to my central question: **will using LLMs to automate machine learning research trigger another revolution in neural network design and training, a significant leap forward?** I want to offer two reasons for skepticism:

**Limitation 1**: Neural architecture search—even LLM-assisted NAS—is only concerned with finding variations of a base architecture. Automatic NAS was already a thing back when LSTMs, ResNets and other CNNs were the most sophisticated architectures around. Researchers then merely looked for different ways of combining and parameterizing known neural network building blocks. There was no chance to find a fundamentally new architecture, like transformers, because the search space only included CNNs to begin with. 

Using LLMs does not necessarily solve this problem. ASI-Arch uses an established baseline architecture as a seed to do some local search around it. While the transformers paper included lots of incremental experiments, too, the initial impulse to go into this direction was based on an *insight*—specifically, that RNNs were not good at modelling long distance dependencies in sequences. This prompted the authors to throw recurrence out and fully embrace self-attention. Transformers were not the mere product of incremental work and local search; they were driven by a conceptual breakthrough that enabled researchers to escape the local optimum of RNNs.

**Limitation 2**: Humans are only so smart—why should an imperfect copy be better? The AI scientist and ASI-Arch use LLMs trained on human language to propose and evaluate papers and implementations. Not only does this risk reproducing human biases for what counts as good research, a good idea, etc.—it imposes a hard limit on the complexity of what these models can generate and evaluate. LLMs are compressed human knowledge. Their ability to generate “new” ideas stems from recombining representations of what humans have already discovered. This becomes a severe limitation when the next breakthrough may require pushing past the envelope of human language. Even if this approach is good enough for the next leap, it likely lacks the capacity to sustain an exponential rate of innovation and growth of complexity.

This is where the analogy to Move 37 becomes instructive. AlphaGo’s move was not derived from human priors—it emerged from the system’s internal representation of the game space, constructed by reinforcement learning with self-play, not by imitation of humans. If this analogy holds, a truly capable, superhuman AI scientist would have to build its own representation of the vast space of scientific theories almost from scratch. How could this work? Human science is deeply rooted in physical experience, abstraction and insight through analogy. Bootstrapping a scientist without a robust perception-abstraction interface appears to be a non-starter then. Further, continual learning is a fundamental problem of neural networks. Training generative models on their own outputs additionally bears the risk of a mode collapse where the models output fidelity and diversity degrade due to the amplification of model artifacts.

In summary, while research in automated search over vast spaces is very interesting[] and will produce some useful tools, I believe that there are fundamental problems with the idea of self-improving LLMs. Their dependence on human priors, lack of a robust real-world interface, as well as their inability for continual learning seem too severe to make them a good candidate for a system capable of exponential self-improvement.


---

<a id="footnote-1">1</a> Another agent named [Carl](https://www.autoscience.ai/blog/meet-carl-the-first-ai-system-to-produce-academically-peer-reviewed-research) published, too, but with a little help of his human friends.

<a id="footnote-2">2</a> or will be occuring [soon](https://www.theguardian.com/technology/2025/jul/26/competition-shows-humans-are-still-better-than-ai-at-coding-just)

<a id="footnote-3">3</a> Linear attention does scale linearly with input sequence length unlike the square scaling of the classical self attention of the transformer. An excellent explanantion can be found [here](https://proceedings.neurips.cc/paper_files/paper/2024/file/e618724ac897c6cf3fbfb273f8695d67-Paper-Conference.pdf).

<a id="footnote-4">4</a>
