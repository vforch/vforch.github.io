---
layout: post
title: "The Complexity Limit of LLMs as Innovation Engines for Machine Learning - Can we find Move 37 for Neural Architecture Search?"
date: 2025-10-01
tags: [complexity, evolution, LLMs]
---

The use of LLMs for innovation may boost productivity in some domains, but can they generate the breakthrough insights needed to revolutionize machine learning itself?

---

Since 2022, LLMs have shown some impressive or at least unexpected capabilities. Their "thinking" or "reasoning" modes, which can be understood as a self-guided shallow search over possible responses, further boosted their capabilities. Recently, LLMs have been used as heuristics for generating and evaluating candidate solutions in large search spaces embedded in the language domain. This approach has already produced some systems with potential real-world impact:

- Using a mix of proprietary LLMs for literature search and information extraction, researchers created an [automated workflow for systematic medical reviews](https://www.medrxiv.org/content/10.1101/2025.06.13.25329541v2). These reviews are very labour-intensive, taking months to complete. The automated workflow, called otto-SR, reduces the time it takes to write such a review to hours (given some detailed, pre-specified inclusion criteria for papers), cutting delays in knowledge accumulation and allowing for frequent updates. Further, across the twelve reviews the authors reproduced, their automatic approach identified more relevant studies and was more reliable at data extraction than human reviewers.
- In an apparent world first, Sakana's AI scientist "published" its [first peer-reviewed paper](https://sakana.ai/ai-scientist-first-publication/).<sup>[1](#footnote-1)</sup> The algorithm employs multiple automatic evaluation functions—including LLM judges—to iteratively improve its LLM-generated research ideas as well as their realization in code, figures, and text. While far from flawless, the paper was deemed "above acceptance threshold" for a workshop paper at a leading machine learning conference. This signifies a serious shift for the scientific community where prestige (and therefore tenure and funding) is largely traded in publication scores. After all, GPU-powered LLMs will likely scale faster than LLM-assisted researchers.

Many of such mini-breakthroughs are currently occuring<sup>[2](#footnote-2)</sup> in domains that interface with language. Since machine learning is mostly underpinned by writing high-level algorithmic descriptions of neural network inference and training as program code, this raises the prospect of letting LLMs re-write their own code, automating most of machine learning research and thus leading to self-improving models. In theory, this could culminate in an AI "singularity"—a point where machine intelligence *suddenly* outclasses human intelligence. If AI were then to spill over into the real world, this would have unpredictable consequences for humanity and life on earth.

The idea of self-replicating and self-improving machines first emerged in the [19th century](https://arxiv.org/pdf/1806.01322), fueled by Charles Darwin's new theory of Evolution through natural selection and the Industrial Revolution's displacement of human labor with tireless machines. If nature—a mindless force—could produce ever-increasing complexity and adaptation and human tasks could be taken over by machines, why couldn't machines start to build ever-improving machines? This line of thinking resurfaced in early science fiction<sup>[3](#footnote-3)</sup> and was later popularised by Ray Kurzweil's 2005 book "The Singularity Is Near", which argued that exponential improvements in available compute (Moores law etc.) would directly accelerate the algorithmic search for better AI. AI superintelligence (ASI) also found its way into academic thinking - first as [speculation](https://doi.org/10.1016/S0065-2458(08)60418-0) and later [formalized](https://people.idsia.ch/~juergen/goedelmachine.html) by Jürgen Schmidhuber as the Gödel machine, which must first prove the optimality of every potential self-improvement. Since the release of ChatGPT, media coverage on AI [exploded](https://doi.org/10.1038/s41562-024-02026-z) and narratives about ASI entered the main stream, [overshadowing other critical concerns](https://doi.org/10.1007/s10676-024-09758-6).

However, not everybody believes in ASI. [Francois Chollet](https://youtu.be/Bo8MY4JpiXE?t=115), for example, disputes the underlying narrative of exponential self-improvement, arguing that there are bottlenecks, diminishing returns and fundamental forces of friction even super smart machines cannot circumvent. Others, like [Yann Lecun](https://www.youtube.com/watch?v=4__gg83s_Do) and [Richard Sutton](https://www.youtube.com/watch?v=21EYKqUsPfg), deny that current transformer-based LLMs could ever scale to anything approaching human intelligence. Matters are further complicated by an unclear definition of the goal state. How to define super intelligence - or human intelligence for that matter - and do we even want something that resembles our own intelligence? 

I think the debate around AI narratives and their societal impact is extremely important<sup>[4](#footnote-4)</sup>, but it's not the point of this post. So let's go back to what is on the horizon in the automatic search department. Sakana also introduced a self-rewriting LLM agent called the [Darwin Gödel Machine](https://sakana.ai/dgm/), which blends Schmidhuber's theoretically optimal but impractical Gödel machine with the fuzziness of evolutionary search. Rather than proving that a change to its codebase is an improvement, the DGM uses empricial performance estimates to decide which changes to make. To avoid local optima, it maintains a repertoire of different "sub-optimal" solutions. This deliberate diversity allows it to make some "detours" in the optimization landscape that eventually lead to better outcomes. While the DGM has been tested only on two coding benchmarks, the results lend some credence to the idea of self-improvement: the DGM outperformed a control setup where a static meta-agent edited a coding agent but couldn't change its own code. However, even in the DGM, the LLM used for code editing and making test-time inferences was frozen—it could not be changed by the agent. Retraining an LLM every time you change its architecture is currently far too expensive to be done at scale.

At a smaller scale, however, algorithmic design of neural network architectures and their training is possible. This field, called [neural architecture search](https://www.jmlr.org/papers/v20/18-598.html) (NAS), is the attempt to automate what researchers in the machine learning community are doing—often manually by trial and error<sup>[5](#footnote-5)</sup>. Of course, NAS, too, is increasingly done using LLMs. A research group based in Shanghai even claims to have created an LLM-guided process capable of finding novel neural architectures humans could not have come up with—analogous to [AlphaGo's Move 37](https://youtu.be/WXuK6gekU1Y?t=2969) which first appeared to be a mistake to the human observer but proved to be a new, ingenious variant humans had never considered during centuries of playing Go. Embracing the singularity narrative, they named their approach [Artificial Superintelligence for AI research](https://arxiv.org/abs/2507.18074) (ASI-Arch). 

ASI-Arch leverages LLMs to propose, implement, and test incremental modifications to explore a search tree over model architectures—similar to what the AI scientist does to produce research papers. Investing merely 20.000 GPU hours—roughly equivalent to the monthly salary of a research scientist in electricity cost—they discovered over 100 new, functioning model architectures with linear attention mechanisms<sup>[6](#footnote-6)</sup>. Interestingly, the scoring metric included not just raw benchmark performance but also an LLM-based rating of the architectural "merit": its novelty, complexity, and speed of convergence during training. To make the search computationally feasible, the tested models were constraint to 340 million parameters—less than 1/1000th the size of proprietary LLMs. However, the [breakthrough transformer paper](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) that launched the LLM era also used models roughly this size. So it is conceivable that the automatic search for new architectures may lead to a new generation of LLMs, right? 

This brings us to my central question: **will using LLMs to automate machine learning research trigger another revolution in neural network design and training, a significant leap forward?** I want to offer two reasons for skepticism:

#### Limitation 1: *Neural architecture search is only concerned with finding variations of a base architecture.* 

NAS was already a thing back when ResNets and other CNNs were the most sophisticated architectures around. Researchers then merely looked for different ways of combining and parameterizing known neural network building blocks. There was no chance to find a fundamentally new architecture, like transformers, because the search space only included CNNs to begin with. Using LLMs does not necessarily solve this problem. ASI-Arch uses an established baseline architecture as a seed to do some local search around it. While the transformers paper included lots of incremental experiments, too, the initial impulse to go into this direction was based on an *insight*—specifically, that RNNs were not good at modelling long distance dependencies in sequences. This prompted the authors to throw recurrence out and fully embrace self-attention. Thus, transformers were not the mere product of incremental work and local search; they were driven by a conceptual breakthrough that enabled researchers to escape the local optimum of RNNs. I am not sure whether diversity-based approaches are enough to allow something akin to a conceptual leap.

#### Limitation 2: *Humans are only so smart—why should an (imperfect) copy be better?* 

The AI scientist and ASI-Arch use LLMs trained on human language to propose and evaluate papers and implementations. Not only does this risk reproducing human biases for what counts as good research, a good idea, etc.—it imposes a hard limit on the complexity of what these models can generate and evaluate. LLMs are compressed human knowledge. Their ability to generate “new” ideas stems from recombining representations of what humans have already discovered. This becomes a severe limitation when the next breakthrough may require pushing past the envelope of human language. Even if this approach is good enough for the next leap, it likely lacks the capacity to sustain an exponential rate of innovation and growth of complexity.

This is where the analogy to Move 37 becomes instructive. AlphaGo’s move was not derived from human priors—it emerged from the system’s internal representation of the game space, constructed by reinforcement learning with self-play, not by imitation of humans. If this analogy holds, a truly capable, superhuman AI scientist would have to build its own representation of the vast space of scientific theories almost from scratch. How could this work? Human science is deeply rooted in physical experience, abstraction and insight through analogy. Bootstrapping a scientist without a robust perception-abstraction interface appears to be a non-starter then. Further, [continual learning is a fundamental problem](https://doi.org/10.1038/s41586-024-07711-7) of neural networks. Training generative models on their own outputs additionally bears the risk of a [mode collapse](https://arxiv.org/abs/2311.16822) where the model's output fidelity and diversity degrade due to the amplification of model artifacts.

In summary, while research in automated search over the vast spaces of all possible languages is very interesting in itself<sup>[7](#footnote-7)</sup> and will produce some useful tools, I believe that there are fundamental problems with the idea of self-improving LLMs given the current state of technology. Their dependence on human priors, lack of a robust real-world interface, as well as their inability for continual learning seem too severe to make them a good candidate for a system capable of exponential self-improvement.


---

<a id="footnote-1">1</a> Another agent named [Carl](https://www.autoscience.ai/blog/meet-carl-the-first-ai-system-to-produce-academically-peer-reviewed-research) published, too, but with a little help of his human friends.

<a id="footnote-2">2</a> ...or will be occuring [soon](https://www.theguardian.com/technology/2025/jul/26/competition-shows-humans-are-still-better-than-ai-at-coding-just).

<a id="footnote-3">3</a> For example [Rossum's Universal Robots](https://en.wikipedia.org/wiki/R.U.R.).

<a id="footnote-4">4</a> Not to mention the potentially detrimental impact of LLM usage on [productivity](https://arxiv.org/abs/2507.09089) and [human cognition](https://arxiv.org/abs/2506.08872).

<a id="footnote-5">5</a> Just like [Evolution](https://arxiv.org/abs/2205.10320).

<a id="footnote-6">6</a> Linear attention does scale linearly with input sequence length unlike the square scaling of the classical self attention of the transformer. An excellent explanantion can be found [here](https://proceedings.neurips.cc/paper_files/paper/2024/file/e618724ac897c6cf3fbfb273f8695d67-Paper-Conference.pdf).

<a id="footnote-7">7</a> Who doesn't want to explore [the space of all possible simulations](https://sakana.ai/asal/)?
