---
layout: post
title: "The Braincraft Challenge by Nicholas Rougier - Computational Neuroscience's ARC AGI Challenge"
date: 2025-07-06
tags: [braincraft, neuroscience]
---

Nicholas Rougier launched a [braincrafting challenge](https://github.com/rougier/braincraft) which I find shares some similarities with Francois Chollet's [ARC AGI challenge](https://arcprize.org/).

The first Braincraft task (of a series of presumably increasingly complex tasks) presents researchers from computational neuroscience with a conspicuously simple problem: a simplified circular robot controlled by a neural network has to navigate a small figure eight-shaped maze where one side contains an energy source. The robot is constantly moving (as long as it's not hitting a wall) and needs to visit the energy source as often as possible to avoid running out of energy. Controls are simple as well: the robot can only steer left or right with a continuous control signal provided by the neural network. The network is made up of up to 1000 continuous rate-coded neurons which can receive input from an array of 64 distance sensors placed in the front of the robot, a binary signal indicating a wall collision, the robot's current energy level, and a constant input. 

{% include figure.html
   src="/assets/img/braincraft_1_small.png"
   alt="Gameplay footage"
   caption="Fig. 1  The robot \"sees\" only in a relatively narrow band in front of it."
   class="my-figure"
%}

The environment appears very similar to rat mazes which have been used by experimental psychologists and later neuroscientists for over a [century](https://en.wikipedia.org/wiki/W._S._Small#Implications_of_maze_learning_and_rats). By using careful experimental design, ever more sophisticated brain imaging techniques, statistical models and simulations, neuroscience as a whole strives for understanding how neurons implement minds, but Rougier laments that "we still lack an integrated, functional mini-brain". By "Mini-brains", in this context, he does not refer to lumps of neurons grown in a [dish](https://doi.org/10.1016/j.neuron.2022.09.001) but neural network models *in silico*. Rougier attests (and I, for one, agree) that most models in computational neuroscience are too specialized and geared towards abstract settings and thus can't be employed in scenarios involving dynamical control and embodiment. Effectively, these models are neither integrated with a body nor a greater environment, but functionally isolated - they are built for solving a narrowly defined task, not for complex settings requiring flexible integration of multiple functions.

But how could this simple task pose difficulties for the status quo? While it's called the "Simple decision" task - in reference to the choice between left and right side of the maze - this task also incorporates problems of dynamical control. First, the controller does not have perfect information - the robot lacks any immediate sense of orientation, the distance sensors are covering only the central 60Â° of the robot's front, and the collision sensor does not inform about *where* the collision happened. This will lead to situations where the robot is close to a wall on its side where it won't have any sensory information of the wall. And when it collides, there will be no straight forward way to get unstuck, since it cannot know where the wall is located. Thus, one major challenge is to give the robot a memory or a spatial representation of its surroundings so it can avoid obstacles that are currently not visible.

{% include figure.html
   src="/assets/img/braincraft_2_small.png"
   alt="Gameplay footage of cutting a corner"
   caption="Fig. 2  There's no way to tell from immediate sensory input whether the robot should turn left or right in this situation."
   class="my-figure"
%}

Giving neural networks some spatial memory of course is not an excessive demand. In computational neuroscience, there are many models for navigation and spatial memory. One recent publication exactly explores the coding limits of small networks consisting of "[a handful of neurons](https://doi.org/10.1038/s41593-024-01766-5)" for spatial navigation which can be examined in the living fruit fly - a system whose computational structure is [relatively well understood](https://doi.org/10.1016/j.conb.2021.12.001). However, as far as I can tell, research in computational neuroscience and areas with more real-world impact like [robotics](https://doi.org/10.1126/scirobotics.abn1944) is mostly disjoint and models from computational neuroscience have been rarely tested in settings where they are used to steer actual behavior. Even more importantly, these models are often hardcoded or fitted to neural activity but not [behavior](http://dx.doi.org/10.1016/j.neuron.2016.12.041), so they are not even meant to learn from environment interactions. 

With the Braincrafting challenge, however, contestants are not allowed to directly submit a optimized network but the *training procedure* for generating one. In a sense, developers need to play Evolution to come up with a mechanism for creating an adaptive system almost from scratch. Further, training time is limited to the equivalent of 100 seconds of compute on a 2020 MacBook Pro. Thus, the challenge lies not in designing a network architecture that could solve this task with unlimited optimization time (through training on data or hand crafting), but finding one that is able to learn to solve the task with extreme efficiency. Judging by the data provided on the Braincraft page - and without considering the computational budget necessary for actual training - experience sampling alone will yield no more than 20 successful episodes or about 25.000 state transitions - which is very little training data even for [state of the art reinforcement learning](https://doi.org/10.48550/arXiv.2111.00210).[1](#footnote-1)

This is where the comparison with the ARC AGI challenge comes to mind, as it also puts emphasis on the available training data. At its core sits the formal (yet incomplete) [definition of intelligence by Francois Chollet](https://doi.org/10.48550/arXiv.1911.01547) as *the rate of information-to-skill transfer*. Under this definition, a system can be said to be intelligent if it is able to adapt to new tasks after minimal exposure. (More formally, Chollet invokes the idea of a "skill programm" - an artifact created by the intelligent system that interacts with the task). Task performance in itself is not necessarily a marker of an intelligent system, since it could also be relying on combinatorial brute force, rote memorization, or the intelligence of the creator of the system. With these insights in mind, Chollet built the ARC benchmark to point out the weaknesses of current machine learning approaches which require internet-scale training data to achieve human level performance in narrowly defined tasks. In ARC, tasks vary a lot, each task being a unique combination of simple concepts which need to be inferred from two to three demonstrations (see Fig. 3). While the first iteration of the ARC challenge was "solved" with massive amounts of compute by OpenAI models (they solved the tasks, but did so very inefficiently), the recently released second iteration seems to be extremely hard even for current models, which average a 2% success rate.[2](#footnote-2) Chollet's Benchmark doesn't only highlight the shortcomings of current AI but also sets a goal for future research and stimulates the discussion about what intelligence actually is.

{% include figure.html
   src="/assets/img/braincraft_3.png"
   alt="ARC task example from Chollet's paper"
   caption="Fig. 3  An ARC task involving the concepts of similarity and count."
%}

Regarding the amount of available training data and adaptability, Rougier now plays a similar game with computational neuroscience models. But what can be gained from solving the Braincraft challenge? Is it even possible to find a meaningful solution to the "simple" task given so little compute? Compared to ARC, I think that Rougier's challenge runs the risk of encouraging "meta-overfitting" to the tasks since there are only 5 different tasks planned for this challenge. One route to success could then be to find a brute force solutions in an extended training regime and compress them by any means so they can be "unpacked" during training using training data as the "decryption key" (a process one could call learning). Cast this way, this appears to be the only approach available - since there is so little data available during training, I doubt that a network could learn a "world model" from scratch. Rather, solutions will have to rely on very strong inductive biases derived from either human intuitions or prior training. This could still leave us with an interesting race - human intuitions vs brute force optimization - whose solution is more suitable to being unpacked during training, and what are the techniques people will use for this? 

I am curious which approaches will be fielded and which ones will succeed in this challenge. Computational neuroscientists, researchers from neuromorphic computing, and advocats for neuro-inspired AI often cite the brain's energy and learning efficiency as an argument against mainstream machine learning and for more brain research. If it turns out that models from the computational neuroscience camp fail in this challenge favouring learning efficiency, this will definitely be a [bitter lesson](http://www.incompleteideas.net/IncIdeas/BitterLesson.html).

---

<a id="footnote-1">1</a> An elaborate training algorithm will strain the available compute budget, reducing the total amount of training data that can be effectively used.

<a id="footnote-2">2</a> Still, once the ML community set eyes on the ARC 1 challenge, progress was fast, and I think rather sooner than later we will find ourselves in a situation where we will be hardpressed to find a purely "cognitive" benchmark machines cannot solve. [Our niche is shrinking.](https://youtube.com/watch?v=dNrTrx42DGQ&t=4637)
